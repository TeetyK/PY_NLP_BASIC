{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec49c06f",
   "metadata": {},
   "source": [
    "# Learning with PyThaiNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27addb8",
   "metadata": {},
   "source": [
    "**Text Preprocessing Pipeline**\n",
    "1.Tokenization แบ่งคำ หน่วยย่อยๆ เรียกว่า Tokens ส่วนคำจะเรียกว่า Words , เครื่องหมายวรรคตอน หรือสัญลักษณ์อื่นๆ\n",
    "|ภาษา|ข้อความเดิม|Token|\n",
    "|---|---|---|\n",
    "|english|I love NLP.| ```I , love , NLP , .```|\n",
    "|thai|ฉันชอบเรียน NLP| ```ฉัน , ชอบ , เรียน , NLP```|\n",
    "\n",
    "2.Normalization การปรับรูปแบบข้อความให้เป็น มาตรฐานเดียวกัน เพือให้คอม มองว่าเป็น คำเดียวกันแต่เขียนต่างกัน เช่น Case Folding เปลี่ยน Lowercase , Uppercase เช่น NLP , nlp มองเป็น nlp\n",
    "\n",
    "3.Stop word removal กำจัดคำที่ไม่สำคัญ คือ การลบคำที่ไม่ค่อยมีความหมายต่อการวิเคราะห์ออกไป\n",
    "English -> the , a , is , of ,and , in\n",
    "Thai -> คือ , อยู่ , ที่ , นี้ , ฉัน , เรา , ของ\n",
    "\n",
    "4.Stemming & Lemmatization หาคำมูลและรากศัพท์\n",
    "\n",
    "Stemming คือตัดส่วนท้ายของคำออกตามกฏทั่วไป อาจจะไม่มีความหมายจริง\n",
    "running -> runn\n",
    "\n",
    "Lemmatization แปลงคำให้อยู่รูปศัพท์ตั้งต้นหลักไวยากรณ์ ได้ความหมายจริง\n",
    "ran , running -> run\n",
    "is , are , was -> be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dbd77",
   "metadata": {},
   "source": [
    "NLP มี Tool ดังนี้\n",
    "NLTK พื้นฐาน ได้ -> Tokenization , Stop Word Removal , Stemming/Lemmatization,POS Tagging\n",
    "\n",
    "SpaCy เน้นประสิทธิภาพความเร็วสูง เหมาะกับงาน Production -> Tokenization , NER , Dependency Parsing\n",
    "PyThaiNLP สำหรับภาษาไทย -> Tokenization คำไทย , Normaliation จัดการวรรณยุกต์ และสระเกิน\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5b7930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token : ['ผม', 'กำลัง', 'วิ่ง', 'ใน', 'สวน', 'รถไฟ', 'ตอนเช้า', 'แล้วก็', 'เผลอ', 'ทำ', 'โทรศัพท์มือถือ', 'ราคา', ' ', '2,000,000', ' ', 'บาท', ' ', 'ตกหาย', 'ไป']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp import word_tokenize\n",
    "\n",
    "text = \"ผมกำลังวิ่งในสวนรถไฟตอนเช้าแล้วก็เผลอทำโทรศัพท์มือถือราคา 2,000,000 บาท ตกหายไป\"\n",
    "\n",
    "tokens = word_tokenize(text,engine=\"newmm\")\n",
    "print(\"Token :\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7be956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token (ลบช่องว่าง) ['ผม', 'กำลัง', 'วิ่ง', 'ใน', 'สวน', 'รถไฟ', 'ตอนเช้า', 'แล้วก็', 'เผลอ', 'ทำ', 'โทรศัพท์มือถือ', 'ราคา', '2,000,000', 'บาท', 'ตกหาย', 'ไป']\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "tokens_filtered = [t for t in tokens if t.strip()]\n",
    "print(\"Token (ลบช่องว่าง)\", tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d6b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Number : ['ผม', 'กำลัง', 'วิ่ง', 'ใน', 'สวน', 'รถไฟ', 'ตอนเช้า', 'แล้วก็', 'เผลอ', 'ทำ', 'โทรศัพท์มือถือ', 'ราคา', '<NUM>', 'บาท', 'ตกหาย', 'ไป']\n"
     ]
    }
   ],
   "source": [
    "# Handle Number\n",
    "import re\n",
    "def normalize_numbers(tokens):\n",
    "    normalized = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'^-?(\\d{1,3}(,\\d{3})*|\\d+)(\\.\\d+)?$',token):\n",
    "            normalized.append('<NUM>')\n",
    "        else:\n",
    "            normalized.append(token)\n",
    "    return normalized\n",
    "\n",
    "tokens_normalized = normalize_numbers(tokens_filtered)\n",
    "print(\"Token Number :\", tokens_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df76e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Stop word :\n",
      "['ผม', 'วิ่ง', 'สวน', 'รถไฟ', 'ตอนเช้า', 'แล้วก็', 'เผลอ', 'ทำ', 'โทรศัพท์มือถือ', 'ราคา', '<NUM>', 'บาท', 'ตกหาย']\n"
     ]
    }
   ],
   "source": [
    "# Stop Word Removal\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "stop_words = thai_stopwords()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "tokens_no_stop = remove_stopwords(tokens_normalized)\n",
    "print(\"Token Stop word :\")\n",
    "print(tokens_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2657da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token POS Tag\n",
      "[('ผม', 'PPRS'), ('วิ่ง', 'VACT'), ('สวน', 'NCMN'), ('รถไฟ', 'NCMN'), ('ตอนเช้า', 'ADVN'), ('แล้วก็', 'JCRG'), ('เผลอ', 'NCMN'), ('ทำ', 'VACT'), ('โทรศัพท์มือถือ', 'NCMN'), ('ราคา', 'NCMN'), ('<NUM>', 'PUNC'), ('บาท', 'NCMN'), ('ตกหาย', 'NCMN')]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization แปลงกลับไปรากศัพท์ โดยมองว่าเป็นความหมายเดียวกัน\n",
    "from pythainlp.tag import pos_tag\n",
    "pos_tags = pos_tag(tokens_no_stop,engine='perceptron')\n",
    "print(\"Token POS Tag\")\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a9920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Frequency",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3e40ef69-645a-494e-a056-604f4f830dd2",
       "rows": [
        [
         "0",
         "ผม",
         "1"
        ],
        [
         "1",
         "วิ่ง",
         "1"
        ],
        [
         "2",
         "สวน",
         "1"
        ],
        [
         "3",
         "รถไฟ",
         "1"
        ],
        [
         "4",
         "ตอนเช้า",
         "1"
        ],
        [
         "5",
         "แล้วก็",
         "1"
        ],
        [
         "6",
         "เผลอ",
         "1"
        ],
        [
         "7",
         "ทำ",
         "1"
        ],
        [
         "8",
         "โทรศัพท์มือถือ",
         "1"
        ],
        [
         "9",
         "ราคา",
         "1"
        ],
        [
         "10",
         "<NUM>",
         "1"
        ],
        [
         "11",
         "บาท",
         "1"
        ],
        [
         "12",
         "ตกหาย",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ผม</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>วิ่ง</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>สวน</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>รถไฟ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ตอนเช้า</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>แล้วก็</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>เผลอ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ทำ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>โทรศัพท์มือถือ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ราคา</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;NUM&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>บาท</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ตกหาย</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Frequency\n",
       "0               ผม          1\n",
       "1             วิ่ง          1\n",
       "2              สวน          1\n",
       "3             รถไฟ          1\n",
       "4          ตอนเช้า          1\n",
       "5           แล้วก็          1\n",
       "6             เผลอ          1\n",
       "7               ทำ          1\n",
       "8   โทรศัพท์มือถือ          1\n",
       "9             ราคา          1\n",
       "10           <NUM>          1\n",
       "11             บาท          1\n",
       "12           ตกหาย          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequency-Based Methods\n",
    "# Bag-of-Word (Bow)\n",
    "# เอกสารเป็นถุก โดยไม่สนใจลำดับของคำ แต่สนใจว่าคำนั้น ปรากฏมากี่ครั้ง\n",
    "import pandas as pd\n",
    "bags_word = {}\n",
    "for i in pos_tags:\n",
    "    if i[0] not in bags_word:\n",
    "        bags_word[i[0]] = 1\n",
    "    else:\n",
    "        bags_word[i[0]] +=1\n",
    "df_word = pd.DataFrame(list(bags_word.items()),columns=['Word','Frequency'])\n",
    "display(df_word) \n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# ตล้าย BOW แต่จะทำการ ถ่วงน้ำหนัก ความถี่ของคำเพื่อเน้นคำที่สำคัยต่อเอกสาารนั้นๆ \n",
    "# TF-IDF มีส่วนแรก term frequency (TF) = TF(t,d) = จำนวนคร้ังที่ ค่า t ปรากฏในเอกสาร d\n",
    "# Inverse Document Frequency (IDF) คือค่าน้ำหนักผกผัน ให้กับคำ ที่ปรากฏหลายๆเอกสาร คำทั่วไป\n",
    "# จะมีค่า IDF ต้ำและค่าที่เอกสารเดี่ยวๆ ค่าเฉลี่ยจะมี สูง\n",
    "# IDF(t,D) = log(จำนวนเอกสารทั้งหมด/จำนวนเอกสารที่มี่ ค่า t ปรากฏ)\n",
    "# TF-IDF คือ การนำ TF มาคูณ กับ IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#แปลงความหมาย Semantic-Based Method\n",
    "# เน้น word embedding\n",
    "# Word2Vec & GloVe\n",
    "# คำที่ปรากฏความหมายใกลเคี้ยงกัน\n",
    "# Word2Vec 1,Continuous Bag-of-Words ใช้คำโดยรอบข้างเพื่่อทำนายคำตรงกลาง 2,Skip-gram ใช้คำตรงกลางเพื่อทำนายรอบข้าง\n",
    "# GloVe ใช้สถิติร่วมกับการเรียนรู้เชิงลึก โดยสร้าง vector จาก อัตราส่วนของการปรากฏร่วมกัน Co-occurrnece Matrix\n",
    "# | คำ | vector 3d | meaning |\n",
    "# | โทรศัพท์มือถือ | [0.8,-0.3,0.1]| (อุปกรณ์, อิเล็กทรอนิกส์ , พกพา)\n",
    "# | แท็ยแล็ต | [0.7,-0.4,0.2]| (อุปกรณ์, อิเล็กทรอนิกส์ , พกพา)\n",
    "# | วิ่ง | [-0.5,0.9,-0.1]| (กิจกรรม, กริยา , เคลือนไหว)\n",
    "# Contextualized Embeddings (BERT/LLMs)\n",
    "# ใช้ architecture Transformer , attention Mechanism เพื่อสร้าง vector\n",
    "# BERT from transformer ไปข้อความทั้งหมดไปหน้า และย้อนกลับเพื่อสร้าง เวกเตอร์\n",
    "# LLM -> GPT | T5 อาศัย Transformer และ Contextual Embeddings เพื่อประมวลภาษา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03dfdf",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b03c85db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "corpus = [\n",
    "    'สินค้าดีมาก การจัดส่งรวดเร็วทันใจ บริการดีเยี่ยม',\n",
    "    'รสชาติกาแฟอร่อย แต่ราคาแพงไปนิดหน่อย',\n",
    "    'การบริการแย่มาก พนักงานหน้าบึ้ง ไม่ประทับใจเลย',\n",
    "    'พัสดุถูกส่งมาตามกำหนดเวลา ไม่มีปัญหาอะไร',\n",
    "    'สถานที่สวยงาม บรรยากาศดี เหมาะกับการพักผ่อน',\n",
    "    'ฉันคิดว่าแอปพลิเคชันนี้ใช้งานง่ายและเป็นประโยชน์มาก',\n",
    "    'คุณภาพสินค้าต่ำกว่าที่คาดไว้ รู้สึกผิดหวัง',\n",
    "    'หนังเรื่องนี้ธรรมดา ไม่ดีไม่แย่ ดูได้เรื่อยๆ',\n",
    "    'รถยนต์คันใหม่ประหยัดน้ำมันได้ดีมาก ถูกใจจริงๆ',\n",
    "    'มีปัญหาในการใช้งานระบบ แต่ได้รับการแก้ไขรวดเร็ว',\n",
    "]\n",
    "Y = [1,0,2,0,1,1,2,0,1,0]\n",
    "df_text = pd.DataFrame(list(corpus),columns=[\"Text\"])\n",
    "df_text['Label'] = Y\n",
    "# TF-IDF Vector\n",
    "def custom_tokenizer(text):\n",
    "    return word_tokenize(text,engine='newmm')\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "X = vectorizer.fit_transform(df_text['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5401c1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,df_text['Label'],test_size=0.3,random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train , y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "display(y_pred) # โมเดล เดาว่า ลบ ทัศนคติเชิงลบ\n",
    "# 0 กลาง 1 บวก 2 ลบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d079cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    กลาง (0)       0.33      1.00      0.50         1\n",
      "     บวก (1)       0.00      0.00      0.00         2\n",
      "      ลบ (2)       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.11      0.33      0.17         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['กลาง (0)','บวก (1)','ลบ (2)']\n",
    "expected_labels = [0,1,2]\n",
    "print(classification_report(y_test,y_pred,labels=expected_labels,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c1223",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a69a6",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) การระบุเอนทิตี้ที่มีชื่อ\n",
    "NER คือการค้นหาและจำแนก เอนทิที้ที่มีชื่อในข้อความ เช่น ชื่อคน สถานที่ องค์กร เวลา จำนวนเงิน\n",
    "โดย ช่วยทำให้เข้าใจว่า who what where when  ในประโยคทำให้สำคัญต่อการทำ information Extraction (การสกัดข้อมูล) และ Question Answering \n",
    "|Entity|Example|Label|\n",
    "|---|---|---|\n",
    "|PERSON|สมชาย ลิซ่า|PER|\n",
    "|ORG|ธกรุงเทพ , google|ORG|\n",
    "|LOCATION|กรุงเทพ สวนลุม|LOC|\n",
    "|DATE/TIME|วันที่ 25 พฤษภาคม เมือวานนี้|DATE/TIME|\n",
    "|MONEY|20,000 บาท , 5 ล้านดอลลาร์|MONEY|\n",
    "\n",
    "NER model ในปัจจุบัน แทนที่จะใช้ความถี่ BoW/TF-IDF เหมือน Text classification พื้นฐาน งาน NER ต้องใช้โมเดลที่สามารถตวสจจับบริบทได้ เช่น RNN / Bi-LSTM-CRF เป็น\n",
    "โมเดลในอดีต เนื่องจากสามารถอ่านบริบทของคำในลำดับ (Sequence) ได้จากทั้งสองทิศทาง (ไปหน้า - หลัง)\n",
    "Transformer Model (BERT/XLM-R) เป็นวิธีที่ทันสมัยที่สุดในปัจจุบัน โดยใช้ transformer เพื่อสร้าง contextual embeddings ที่แม่นยำสูงและนำไป fine-tune สำหรับงาน NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd40df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER :\n",
      "[\"('คุณสมบัติ', 'O')\", \"('ของ', 'O')\", \"('ท่าน', 'O')\", \"('สามารถ', 'O')\", \"('ใช้', 'O')\", \"('เงิน', 'O')\", \"('ที่', 'O')\", \"('ฝาก', 'O')\", \"('ไว้', 'O')\", \"('กับ', 'O')\", \"('ธนาคาร', 'B-ORGANIZATION')\", \"('กรุงเทพ', 'I-ORGANIZATION')\", \"('ใน', 'O')\", \"('การลงทุน', 'O')\", \"('ใน', 'O')\", \"('ตลาด', 'O')\", \"('หลักทรัพย์', 'O')\", \"('ได้', 'O')\"]\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tag import NER\n",
    "ner = NER()\n",
    "text_thai = \"คุณสมบัติของท่านสามารถใช้เงินที่ฝากไว้กับธนาคารกรุงเทพในการลงทุนในตลาดหลักทรัพย์ได้\"\n",
    "entities = ner.tag(text_thai)\n",
    "print(\"NER :\")\n",
    "print([f\"{i}\" for i in entities])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385588a7",
   "metadata": {},
   "source": [
    "# Text Summarization สรุปแบบดึงมา (Extractive Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b07253f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b26a25a45c4d1db39794c428fabe33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\home\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\home\\.cache\\huggingface\\hub\\models--Nopphakorn--mt5-small-thaisum-512-title. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0ab3d0be8c4f45ae5cc57f992cea53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba95af7143646abb57d062b3c74aaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b23e66445e4ad989e2d34c34d20098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007922abdbe743bcb0994dc54ee25b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a04ef0052b4ecb8fc92bdf0ea87f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f494bcd009486183df591b498d8337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3fd3ec84824d019892f31c24c8b8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ผลลัพธ์ Abstractive Summarization:\n",
      "นายกรัฐมนตรีของไทยเดินทางเข้าร่วมประชุมสุดยอดผู้นําอาเซียนครั้งที่ 43 ด้วยการเน้นย้ําถึงความสําคัญของการค้าเสรีและการลงทุนในภูมิภาคอาเซียน\n"
     ]
    }
   ],
   "source": [
    "# คือการดึงประโยคสำคัญ จากเอกสารต้นฉบับมาเรียงต่อเป็น ข้อความสรุป\n",
    "# วิิธีไม่ได้สร้างประโยคใหม่ แต่เน้นค้นหาประโยคที่มีความสำคัญสูงสุด โดยใช้ Sentence Scoring\n",
    "# 1.TF 2. TF-IDF 3.Sentence Position\n",
    "# ซึ่งมีเทคนิค TextRank\n",
    "# โดยจะสราา้ง Graph Construction มี Nodes แต่ละประโยค และ Edges เส้นเชื่อมระหว่างสอง nodes\n",
    "# Score calcculation node ที่เชื่อมกันจำนวนมาก หรือ node สำคัญ จะได้รับคะแนนสูงกว่า \n",
    "# จะใช้ algorithm วนซ้ำจนกว่าคะแนนของทุก node จะคงที่\n",
    "# สร้างสรุป โดยเลือก node ที่ได้คะแนนสูงสุดตามจำนวนที่ต้องการสรุป เช่น 3 ประโยคแรก\n",
    "from transformers import pipeline\n",
    "\n",
    "# สมมติว่ามีโมเดล Transformer ภาษาไทยที่ถูก Fine-tune มาสำหรับ Summarization\n",
    "# (เช่น 'seonglae/mBart-large-th-summarization' หรือ LLM ตัวอื่น)\n",
    "try:\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\", \n",
    "        model=\"Nopphakorn/mt5-small-thaisum-512-title\", \n",
    "        tokenizer=\"Nopphakorn/mt5-small-thaisum-512-title\"\n",
    "    )\n",
    "    \n",
    "    long_text = \"การประชุมสุดยอดผู้นำอาเซียนครั้งที่ 43 ได้เริ่มต้นขึ้นในกรุงจาการ์ตา อินโดนีเซีย เมื่อวันที่ 5 กันยายน 2566 โดยมีวาระสำคัญคือการเสริมสร้างความร่วมมือทางเศรษฐกิจและการแก้ไขปัญหาวิกฤตการณ์ในเมียนมา นายกรัฐมนตรีของไทยได้เดินทางเข้าร่วมเพื่อเน้นย้ำถึงความสำคัญของการค้าเสรีและการลงทุนในภูมิภาค\"\n",
    "    \n",
    "    # สรุปแบบ Abstractive (เพราะใช้ Transformer)\n",
    "    summary_result = summarizer(long_text, max_length=50, min_length=10, do_sample=False)\n",
    "    \n",
    "    print(\"\\nผลลัพธ์ Abstractive Summarization:\")\n",
    "    print(summary_result[0]['summary_text'])\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n---\")\n",
    "    print(\"ไม่สามารถรันตัวอย่าง Transformer ได้ กรุณาติดตั้งไลบรารี transformers และ PyTorch/TensorFlow\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424ba86",
   "metadata": {},
   "source": [
    "# Basic Word Thai with PyThaiNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fa1aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "pythainlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ตัวอักษรไทย\")\n",
    "print(pythainlp.thai_characters)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_characters))\n",
    "print(\"พยัญชนะไทย\")\n",
    "print(pythainlp.thai_consonants)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_consonants))\n",
    "print(\"สระ\")\n",
    "print(pythainlp.thai_vowels)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_vowels))\n",
    "print(\"วรรณยุกต์\")\n",
    "print(pythainlp.thai_tonemarks)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_tonemarks))\n",
    "print(\"เครื่องหมายที่เกี่ยวข้อง\")\n",
    "print(pythainlp.thai_signs)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_signs))\n",
    "print(\"เครื่องหมายวรรคตอน\")\n",
    "print(pythainlp.thai_punctuations)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_punctuations))\n",
    "print(\"เลขไทย\")\n",
    "print(pythainlp.thai_digits)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_digits))\n",
    "print(\"สัญลักษณ์ สกุลเงินไทย ->\", pythainlp.thai_symbols)\n",
    "print(\"รวมอักษรไทย\")\n",
    "print(pythainlp.thai_letters)\n",
    "print(\"จำนวนตัวอักษร : \",len(pythainlp.thai_letters))\n",
    "print(\"เช็คว่า , อยู่ใน set character ไหม : \",(',' in pythainlp.thai_characters))\n",
    "print(\"เช็คว่า ๑ อยู่ใน set character ไหม : \",('๑' in pythainlp.thai_characters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythainlp.util\n",
    "\n",
    "print(pythainlp.util.isthai(\"ก\"))\n",
    "print(pythainlp.util.isthai(\"สวัสดีตอนเช้า.\"))\n",
    "print(pythainlp.util.isthai(\"สวัสดีตอนเช้า กินข้าวหรือยัง?\"))\n",
    "print(pythainlp.util.isthai(\"สวัสดีตอนเช้า กินข้าวหรือยัง?\",ignore_chars=\"?\"))\n",
    "print(pythainlp.util.isthai(\"สวัสดีตอนเช้า กินข้าวหรือยัง?\",ignore_chars=\" ?\"))\n",
    "print(pythainlp.util.isthai(\"ดัชนีช่วงเช้าปิดที่ระดับ 60,596.53 จุด เพิ่มขึ้น 18.01 จุด หรือ 2.51% มูลค่าการซื้อขาย 8.95 หมื่นล้านบาท\",ignore_chars=\" 01234567890,.%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pythainlp.util.countthai(\"แมวสีสวาท\"))\n",
    "print(pythainlp.util.countthai(\"แมว cat\"))\n",
    "print(pythainlp.util.countthai(\"วันอาทิตย์ที่ 24 มีนาคม 2562\"))\n",
    "print(pythainlp.util.countthai(\"วันอาทิตย์ที่ 24 มีนาคม 2562\",ignore_chars=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60404c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import collate\n",
    "thai_words = [\"หมู\",\"ไก่\",\"ช้าง\",\"ม้า\",\"ควาย\"]\n",
    "collate(thai_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3df335",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate(thai_words,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c461531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ไทย ผสมตัวเลข\n",
    "thai_words = [\"หมูแดง\", \"หมูทอด\", \"หมู 3 ชั้น\", \"หมูกรอบ\", \"หมูปิ้ง\", \"หมูตุ๋น\", \"หมูแฮม\", \"Suckling pig\", \"Pork chop\"]\n",
    "collate(thai_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pythainlp.util import thai_strftime\n",
    "\n",
    "date = datetime.datetime(2020,12,31,23,59)\n",
    "fmt = \"%Aที่ %-d %B พ.ศ. %Y เวลา %H:%M น.\"\n",
    "thai_strftime(date,fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"%d-%b-%y\"\n",
    "thai_strftime(date,fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"%c\"\n",
    "thai_strftime(date,fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"%v\"\n",
    "thai_strftime(date,fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"%Aที่ %-d %B พ.ศ. %Y เวลา %H:%M น. (%a %d-%b-%y)\"\n",
    "thai_strftime(date,fmt,thaidigit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import time_to_thaiword\n",
    "print(time_to_thaiword(\"00:01:19\"))\n",
    "print(time_to_thaiword(\"00:00:01\"))\n",
    "print(time_to_thaiword(\"00:10:00\"))\n",
    "print(time_to_thaiword(\"01:00:00\"))\n",
    "print(time_to_thaiword(\"00:00:00\"))\n",
    "print(time_to_thaiword(\"23:59:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"19:30:01\",fmt=\"24h\"))\n",
    "print(time_to_thaiword(\"19:30:01\",fmt=\"6h\"))\n",
    "print(time_to_thaiword(\"05:00:07\",fmt=\"24h\"))\n",
    "print(time_to_thaiword(\"05:00:07\",fmt=\"6h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab868bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"08:28:59\",fmt=\"6h\"))\n",
    "print(time_to_thaiword(\"08:28:59\",fmt=\"m6h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"11:48:59\",precision=\"m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"11:00:00\",fmt=\"24h\"))\n",
    "print(time_to_thaiword(\"11:00:00\",fmt=\"24h\",precision=\"s\"))\n",
    "print(time_to_thaiword(\"11:00:00\",fmt=\"24h\",precision=\"m\"))\n",
    "print(time_to_thaiword(\"11:00:00\",fmt=\"24h\",precision=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"11:48:00\",fmt=\"6h\"))\n",
    "print(time_to_thaiword(\"11:48:00\",fmt=\"6h\",precision=\"s\"))\n",
    "print(time_to_thaiword(\"11:48:00\",fmt=\"6h\",precision=\"m\"))\n",
    "print(time_to_thaiword(\"11:48:00\",fmt=\"6h\",precision=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_to_thaiword(\"07:00:00\",fmt=\"m6h\"))\n",
    "print(time_to_thaiword(\"07:00:00\",fmt=\"m6h\",precision=\"s\"))\n",
    "print(time_to_thaiword(\"07:00:00\",fmt=\"m6h\",precision=\"m\"))\n",
    "print(time_to_thaiword(\"07:00:00\",fmt=\"m6h\",precision=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3aef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "time = datetime.time(23,59,58)\n",
    "time_to_thaiword(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime(1980,12,31,23,58,57)\n",
    "time_to_thaiword(time,fmt=\"6h\",precision=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eb2678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หนึ่งร้อยยี่สิบสามล้านสี่แสนห้าหมื่นหกพันเจ็ดร้อยแปดสิบเก้า\n",
      "ศูนย์\n",
      "2000000000046722\n",
      "หนึ่งหมื่นสองพันสามร้อยสี่สิบห้าบาทหกสิบเจ็ดสตางค์\n",
      "หนึ่งหมื่นล้านบาทถ้วน\n",
      "ศูนย์บาทถ้วน\n",
      "ศูนย์บาทหนึ่งสตางค์\n",
      "ศูนย์บาทถ้วน\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.util import num_to_thaiword , thaiword_to_num , bahttext\n",
    "print(num_to_thaiword(123456789))\n",
    "print(num_to_thaiword(0))\n",
    "print(thaiword_to_num(\"สองพันล้านล้านสี่หมื่นหกพันเจ็ดร้อยยี่สิบสอง\"))\n",
    "print(bahttext(12345.67))\n",
    "print(bahttext(10000000000.00))\n",
    "print(bahttext(0))\n",
    "print(bahttext(0.01))\n",
    "print(bahttext(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.soundex import lk82 , metasound , udom83\n",
    "print(lk82(\"ค่า\")== lk82(\"ข้า\"))\n",
    "print(lk82(\"การ\")== lk82(\"กาล\"))\n",
    "print(lk82(\"เพชรรัช\")== lk82(\"เพชรรัศม์\"))\n",
    "print(metasound(\"สวิทซ์\")== metasound(\"สวัส\"))\n",
    "print(udom83(\"ค่า\")== udom83(\"ข้า\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad36e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"ค่า\", \"ข้า\", \"ฆ่า\", \"การ\", \"กาล\", \"การณ์\", \"เพ็ชรรัตน์ \", \"เพชรรัตติ์\", \"รัก\", \"รักษ์\"]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"Text: {text}, lk82:, {lk82(text)}, udom83: {udom83(text)}, metasound: {metasound(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cc173",
   "metadata": {},
   "source": [
    "# SpaCy_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f1d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506c3fb",
   "metadata": {},
   "source": [
    "Stemming คึอ กระบวนการตัดส่วนท้ายคำ แบบหยาบๆ Heuristic ซึ่งได้ผล พอตวร สำเหรับ อังกฤษ และทำให้คำลดลง ซึ่งเป็นการตัดแบบอาจจะเสียความหมายไปเลย\n",
    "Lemmatization คือแปลง work ด้วยรายการศัพท์ใน dict ส่วนใหญ่ จะตัดส่วนท้ายของคำให้ คงรูปพื้นฐาน เรียกว่า Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f5f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง WordNetLemmatizer , PorterStemmer เป็น Stemmer , Lemmatizer อังกฤษ\n",
    "from nltk import stem\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604e0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lemma_stem(word_list):\n",
    "    for word in word_list:\n",
    "        print(f'{word:12} ==> {wnl.lemmatize(word):12}\\t{porter.stem(word):12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36567c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot         ==> foot        \tfoot        \n",
      "feet         ==> foot        \tfeet        \n",
      "foots        ==> foot        \tfoot        \n",
      "footing      ==> footing     \tfoot        \n"
     ]
    }
   ],
   "source": [
    "word_list = ['foot', 'feet', 'foots', 'footing']\n",
    "print_lemma_stem(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2480498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly          ==> fly         \tfli         \n",
      "flies        ==> fly         \tfli         \n",
      "flying       ==> flying      \tfli         \n",
      "flew         ==> flew        \tflew        \n",
      "flown        ==> flown       \tflown       \n"
     ]
    }
   ],
   "source": [
    "word_list = ['fly', 'flies', 'flying', 'flew', 'flown']\n",
    "print_lemma_stem(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5968ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organize     ==> organize    \torgan       \n",
      "organized    ==> organized   \torgan       \n",
      "organizing   ==> organizing  \torgan       \n"
     ]
    }
   ],
   "source": [
    "word_list = ['organize', 'organized', 'organizing']\n",
    "print_lemma_stem(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d845e067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe     ==> universe    \tunivers     \n",
      "university   ==> university  \tunivers     \n",
      "universal    ==> universal   \tunivers     \n"
     ]
    }
   ],
   "source": [
    "word_list = ['universe', 'university', 'universal']\n",
    "print_lemma_stem(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bf38ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The          ==> The         \tthe         \n",
      "formatting   ==> formatting  \tformat      \n",
      "operations   ==> operation   \toper        \n",
      "described    ==> described   \tdescrib     \n",
      "here         ==> here        \there        \n",
      "exhibit      ==> exhibit     \texhibit     \n",
      "a            ==> a           \ta           \n",
      "variety      ==> variety     \tvarieti     \n",
      "of           ==> of          \tof          \n",
      "quirks       ==> quirk       \tquirk       \n",
      "that         ==> that        \tthat        \n",
      "lead         ==> lead        \tlead        \n",
      "to           ==> to          \tto          \n",
      "a            ==> a           \ta           \n",
      "number       ==> number      \tnumber      \n",
      "of           ==> of          \tof          \n",
      "common       ==> common      \tcommon      \n",
      "errors.      ==> errors.     \terrors.     \n"
     ]
    }
   ],
   "source": [
    "word_list = \"The formatting operations described here exhibit a variety of quirks that lead to a number of common errors.\".split()\n",
    "print_lemma_stem(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eace01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a999eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lemma(sentence):\n",
    "    sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        print(f'{word.text:12} \\t==> {word.lemma_:12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf6a9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car          \t==> car         \n",
      "cars         \t==> car         \n"
     ]
    }
   ],
   "source": [
    "print_lemma('car cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "450969a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organize     \t==> organize    \n",
      "organized    \t==> organized   \n",
      "organizes    \t==> organize    \n",
      "organizing   \t==> organize    \n",
      "organization \t==> organization\n",
      "organizations \t==> organization\n"
     ]
    }
   ],
   "source": [
    "print_lemma('organize organized organizes organizing organization organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ead9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnning-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
